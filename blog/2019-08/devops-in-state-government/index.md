---
title: Implementing DevOps in State government
description: The story of how I implemented DevOps practices at a State government agency
author: shawn.sesna@octopus.com
visibility: public
bannerImage: 
metaImage: 
published: 
tags:
 - configuration
---
## Implementing DevOps in State government

We’ve all heard the phrase “the speed of government” when describing things that move slowly.  It is true that in a bureaucratic setting, things tend to move at a snail's pace as compared to the rapid and sometimes chaotic environment of a start-up.  Though snails move slowly, they do eventually get where they need to be.  Like the snail analogy, implementing change is possible in government.

Just like start-ups, government agencies suffer from the same issues like standardization of builds, testing, consistent infrastructure, and reliable deployment processes.  The challenge of implementing a concept like DevOps, is change.  Let’s be honest, change is hard, but it doesn’t have to be bad.  In 2011, I was hired as a Configuration Manager at a small State government agency.  I was tasked with automating the manual processes to improve reliability of software delivery, reduce the length of time it took to deliver, and try to eliminate the need to deploy on weekends.  At that time, web code builds were done on developer machines, zipped, and copied to a file share.  Database changes were handled similarly by zipping up a bunch of scripts with a document explaining in which order to run them, then copying them to a file share for the DBAs to pick up.  Inevitably a deployment would fail because a developer neglected to mention there was a third-party dependency that needed to be installed on the web server, the scripts for the database changes weren’t tested to make sure they worked with the current state of the Production database, or good old-fashioned human error.  Things needed to change, badly.

I decided to start at the beginning of the process with the builds so I could eliminate the adage, “worked on my machine, ops problem now.”  The team was already using Microsoft Team Foundation Server for source control so the build controller technology was already present.  I installed the controller as well as a couple of agents so that all software was built against an independent machine.  This quickly brought to light any dependencies that were present on the developer machines that were not available on the servers and would need to be installed.  Though this helped with identifying what would need to be installed on the server, the web admins would still sometimes forget to install the dependencies on all servers and deployment failures still occurred, that issue wouldn’t get resolved until several years later.

Next on the list was to tackle web code deployments.  I learned of Microsoft Web Deploy and wrote a small console application that used it to start consistently deploying the web code.  This required a small change in the build process to produce the zip file in a way that Web Deploy expected.  Utilizing web.config transforms, we were able to get most of the way there with automating web code deployments, but still faced an issue of needing to update connection strings manually.  Not great, but it was a start.

For database code, I wrote another small console application that would read an XML file that specified the order to run the scripts in.  Unlike the manual way of running the scripts, the console application ran the scripts within a single transaction and rolled back in the event of failure.  Not only did this method reduce the error rate, it reduced the time it took for a deployment since the DBAs no longer had to open the scripts one at a time and execute.

With these two console applications, we were able to reduce the failed deployment rate as well as reduce the amount of time it took to complete them.  The skepticism and the “that’s how we’ve always done it” mentality morphed into acceptance.  Weekend deployments weren’t completely eliminated, but they were significantly reduced which made for happier devs and operations folks.

The console applications were eventually combined into an automated deployment solution consisting of a server, agents on target machines, and a web-based interface.  Developers had the ability to pull directly from source control, build, and schedule deployments for a later date and time.  This solution further reduced the failed deployment rate and sped up the deployment process all but eliminating the need for weekend work.  Eventually, the developers outgrew the solution and needed something more than my programming skills were able to deliver.

A contractor happened to be working on a project that used my solution and gave us a demonstration of a tool he’d come across for automating deployments, Octopus Deploy.  After the demonstration, one developer was adamant this is what we needed.  As the proud papa of my solution, I was reluctant to abandon my creation.  However, I eventually came to the realization that it was better for the agency to adopt the tool and shed my own “that’s how we’ve always done it” mindset.  Over the course of a few weeks, I learned how to use the Octopus Deploy product and was able to duplicate the functionality of my solution.  A few months later, my solution performed its last deployment and the adoption of Octopus was complete.

One issue that continued to plague us was inconsistent environments.  I learned of the Infrastructure as Code concept and was immediately on board.  Having no experience in any of the existing technologies (Chef, Puppet, Ansible, PowerShell DSC, etc…), I decided to try PowerShell DSC (Desired State Configuration).  I quickly learned why all of the PowerShell courses say something like, “... and then there’s PowerShell DSC, but that’s whole course in of itself.”  Octopus Deploy gave me some great PowerShell experience, but DSC was definitely a different animal.  After a bit of learning, I was able to demonstrate how to configure a bare metal server (a VM, to be honest) to functional IIS server in minutes.  Not only that, I showed how I was able to combine the deployment power of Octopus Deploy with PowerShell DSC and push out configuration to servers just like an application deployment!  Now that I had the web administrators on board, I turned to the database administrators.  Working with the DBA team, we created a DSC script that would install, configure, and maintain SQL Servers and hooked it up to Octopus as well.  The DBA team was now able to keep tabs on their servers and change things whenever they wanted.  This reduced the friction between the Operations team and the DBA team.

None of this happened overnight.  At this point we’re at the beginning of 2018 and close to the end of my career in State government.  When I left to join Octopus Deploy, I had just started our DevSecOps implementation by utilizing [Contrast Security](https://www.contrastsecurity.com/) and was trialing some monitoring tools such as [New Relic](https://newrelic.com/) and [Stackify Retrace](https://stackify.com/) which would flow into the continuous feedback loop.  While unit testing hadn’t been fully accepted by all development teams, it had become mandatory in all new projects.  Selenium acceptance was in its infancy and not widely used. I remember getting a compliment from a developer, he said that he loved the fact he could do a merge, a build would execute, then it would deploy the code out to the desired environment.  He said he’d click merge, go get coffee, and when he got back, his app was deployed.

## Conclusion
Our pace was slow, just like the snail.  But, like the snail, we were inching our way towards our end goal of being a DevOps shop.  I’ve been gone almost five months from my previous job, but have kept in contact with the team I left behind.  They have continued down the path that I had started.  It may be slow, but implementing change such as the DevOps concept in State government is indeed possible.
